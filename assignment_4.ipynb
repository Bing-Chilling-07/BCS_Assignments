{"cells":[{"cell_type":"markdown","metadata":{"id":"W9dGAiJpfwgc"},"source":["# **TETRIS:**\n","\n","# **Team NEURON BANK**"]},{"cell_type":"markdown","source":["## link to the dueling DQN method we used:\n","https://www.youtube.com/watch?v=3ILECq5qxSk"],"metadata":{"id":"3IC7SrjtqH3z"}},{"cell_type":"markdown","metadata":{"id":"UZj30uHeq8rZ"},"source":["## **installing all the required packages:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MhWzfxz88c9d","outputId":"a560a23c-2b2a-4b2d-97a2-b8f0f130d07e","executionInfo":{"status":"ok","timestamp":1737297515974,"user_tz":-330,"elapsed":2173,"user":{"displayName":"Bing ChillinG","userId":"04782949655228965660"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ale_py\n","  Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale_py) (1.26.4)\n","Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ale_py\n","Successfully installed ale_py-0.10.1\n"]}],"source":["pip install ale_py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ANs2yJ4Uf7Kh","outputId":"fe7e49df-cf6e-4ac5-ad41-3b6444f61676","executionInfo":{"status":"ok","timestamp":1737297518925,"user_tz":-330,"elapsed":2955,"user":{"displayName":"Bing ChillinG","userId":"04782949655228965660"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"]}],"source":["pip install gymnasium\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tnns7H2WgVK1"},"outputs":[],"source":["import gymnasium as gym\n","import ale_py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import random\n","from IPython import get_ipython\n","from IPython.display import display, clear_output\n","from collections import namedtuple, deque\n","from itertools import count"]},{"cell_type":"markdown","metadata":{"id":"CyXHhHGhrFfd"},"source":["## **Creating an environment:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8Q9BuHzI7Hd"},"outputs":[],"source":["from gymnasium import RewardWrapper\n","\n","class TetrisRewardWrapper(RewardWrapper): #modify the reward system of the present tetris game.\n","    def __init__(self, env):\n","        super().__init__(env)\n","\n","    def reward(self, reward):\n","\n","        # Get the state of the board (you may need to extract it depending on your environment)\n","        board = self.get_board_from_observation(self.env.render())  # Adjust based on how your environment exposes the state\n","\n","        # Calculate components of the reward\n","        max_height = self.get_max_height(board)\n","        gap_penalty = self.get_gap_penalty(board)\n","\n","        # Combine rewards\n","     # Retain original reward for clearing lines\n","        modified_reward = reward\n","        modified_reward -= 1.0 * max_height  # Penalize tall stacks\n","        modified_reward -= 0.5 * gap_penalty  # Penalize gaps\n","\n","        return modified_reward\n","\n","    def get_board_from_observation(self, observation):\n","\n","        # Process the observation to extract the board\n","        # NOTE: This will depend on the specific Tetris environment\n","        # For example, you might threshold pixel values to identify blocks\n","        grayscale = np.mean(observation, axis=2)  # Convert to grayscale\n","        board = (grayscale > 128).astype(int)  # Binarize (adjust threshold as needed)\n","\n","        # Crop to the region of the screen containing the Tetris board\n","        # This depends on the screen layout of the Tetris game in ALE\n","        board_region = board[50:210, 30:130]  # Example cropping (adjust as needed)\n","        return board_region\n","\n","    def get_max_height(self, board):\n","        \"\"\"Calculate the maximum height of the stack.\"\"\"\n","        for row in range(len(board)):\n","            if any(board[row]):\n","                return len(board) - row\n","        return 0  # No stack\n","\n","    def get_gap_penalty(self, board):\n","        \"\"\"Calculate the number of gaps in the stack.\"\"\"\n","        gaps = 0\n","        for col in range(len(board[0])):  # Iterate over each column\n","            filled = False\n","            for row in range(len(board)):\n","                if board[row][col]:  # Block present\n","                    filled = True\n","                elif filled:  # Gap below a block\n","                    gaps += 1\n","        return gaps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpFznSnNgyri"},"outputs":[],"source":["env = gym.make(\"ALE/Tetris-ram-v5\", render_mode=\"rgb_array\")\n","env = TetrisRewardWrapper(env)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0m-HB_-kHeW1","outputId":"d86e13e0-3c24-4b7b-fa1e-7574ecb82658","executionInfo":{"status":"ok","timestamp":1737297525466,"user_tz":-330,"elapsed":1319,"user":{"displayName":"Bing ChillinG","userId":"04782949655228965660"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[  1   1   0   0   0  12  12   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0 238 170 170 170 238   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0 119  85  85  85 119   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n"," 126  21   5   5   6   6   4   8   4   8   0   0   0   0   7   6   5   4\n","   4   6   1   2  16   0   0   0  75 255  75 255  15 240   0   0   0   0\n"," 207 252]\n","False\n","False\n","Total reward for random actions: -227127.0\n"]}],"source":["state,_ = env.reset()\n","\n","total_reward = 0\n","for _ in range(100):  # Play 100 steps\n","    action = env.action_space.sample()  # Random action\n","    observations, reward, terminated, truncated, _ = env.step(action)\n","    total_reward += reward\n","    if terminated or truncated:\n","        break\n","print(observations)\n","print(terminated)\n","print(truncated)\n","print(f\"Total reward for random actions: {total_reward}\")\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"6kYzc5e8rKIe"},"source":["## **creating a replay memory class for storing all the episodes and its output:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_I2XrWx3xuWe"},"outputs":[],"source":["transition = namedtuple('transition', ('state', 'action', 'next_state', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","\n","  def __init__(self, capacity):\n","    self.memory = deque([], maxlen=capacity)\n","\n","  def push(self, *arguements):\n","    self.memory.append(transition(*arguements))\n","\n","  def sample(self, batch_size):\n","    return random.sample(self.memory, batch_size)\n","\n","  def length(self):\n","    return len(self.memory)\n"]},{"cell_type":"markdown","metadata":{"id":"emcOwpwxrVLt"},"source":["## **Neural Network class:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUUgekd-x3FC"},"outputs":[],"source":["class DQN(nn.Module):                                                   #for a change, we use dueling DQN method instead of the regular one\n","  def __init__(self, n_obv,n_hidden, n_act, enable_dueling_dqn = True):\n","    super(DQN, self).__init__()\n","\n","    self.enable_dueling_dqn = enable_dueling_dqn\n","\n","    self.hidden = nn.Linear(n_obv, n_hidden)\n","\n","    if self.enable_dueling_dqn:\n","      #value stream\n","      self.fc_value = nn.Linear(n_hidden, 128)\n","      self.value = nn.Linear(128, 1)\n","\n","      #advantages stream\n","      self.fc_advantages = nn.Linear(n_hidden, 128)\n","      self.advantages = nn.Linear(128, n_act)\n","\n","    else:\n","      self.output = nn.Linear(n_hidden, n_act)\n","\n","\n","  def forward(self, x):\n","    x = F.relu(self.hidden(x))\n","\n","    if self.enable_dueling_dqn:\n","      v = F.relu(self.fc_value(x))\n","      V = self.value(v)\n","\n","      a = F.relu(self.fc_advantages(x))\n","      A = self.advantages(a)\n","\n","      Q = V + A - torch.mean(A, dim=1, keepdim=True)\n","\n","    else:\n","     Q = self.output(x)\n","\n","    return Q"]},{"cell_type":"markdown","metadata":{"id":"3wCq3QRYreot"},"source":["## **Initializing all the hyper-parameters:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvUfJjICg2S9"},"outputs":[],"source":["BATCH_SIZE = 64\n","GAMMA = 0.995\n","EPS_START = 0.99\n","EPS_END = 0.05\n","EPS_DECAY = 1000\n","TAU = 0.005\n","LEARN_RATE = 1e-3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v49OYQ2FhtN3"},"outputs":[],"source":["n_act = env.action_space.n                #number of actions in the action space of the environment(4)\n","state, info = env.reset()                 #reset the state and info from the environment\n","n_obv = state.shape[0]                    #number of observations in the state(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXlwMmUsMT9-"},"outputs":[],"source":["online_net = DQN(n_obv, 128, n_act, enable_dueling_dqn = True)       #creating an constantly updating onine_network\n","target_net = DQN(n_obv, 128, n_act, enable_dueling_dqn = True)       #creating a target_network which is soft updated with values from the online_network\n","target_net.load_state_dict(online_net.state_dict())       #initially copying all the weights of the online_network to the target_network\n","\n","optimizer = optim.AdamW(online_net.parameters(), lr = LEARN_RATE, amsgrad = True)         #optimizing the online_network to minimize the loss function"]},{"cell_type":"markdown","metadata":{"id":"frZb7NnWtN9C"},"source":["## **create the action selection function using the epsilon-greedy method:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYZLDTUWVXJJ"},"outputs":[],"source":["replay_memory = ReplayMemory(101010)      #create a replay_memory from the replay memory class with length of 101010\n","import math\n","\n","\n","steps_done = 0\n","def action_selection(state):\n","\n","  global steps_done\n","\n","  sample = random.random()               #generating a random number for sample\n","  eps_treshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * steps_done / EPS_DECAY)            #changing the epsilon value with every step, initially the eps value is equal to eps start, but slowly it will migrate to the eps_end value\n","\n","  steps_done += 1\n","  if sample > eps_treshold:\n","    with torch.no_grad():\n","      return online_net(state).max(1).indices.view(1,1)          #generating the max value action if the sample is greater than the threshold (exploitation)\n","\n","  else:\n","    return torch.tensor([[env.action_space.sample()]], dtype = torch.long)         #generating a random action if the sample is less than the threshold (exploration)\n"]},{"cell_type":"markdown","metadata":{"id":"Dq5pfwfouSyd"},"source":["## **Creating the plot function:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoQwvhjzWbOs"},"outputs":[],"source":["episode_rewards = []   #store the rewards for each episode to plot\n","\n","def plot_rewards(show_result=False):\n","  plt.figure(1)\n","  rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n","\n","  plt.figure(figsize=(14, 10))\n","  if show_result:\n","    plt.title('Result')\n","\n","  else:\n","    plt.clf()\n","    plt.title(\"training . . .\")\n","  plt.xlabel(\"Episode\")\n","  plt.ylabel(\"Reward\")\n","  plt.plot(rewards_t.numpy())\n","\n","  if len(rewards_t) >=100:                                #plot a second mean function\n","    means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n","    means = torch.cat((torch.zeros(99), means))\n","\n","    plt.plot(means.numpy())\n","\n","\n","  plt.pause(0.01)\n","\n","  if get_ipython() is not None:\n","    if not show_result:\n","      print(\"Training . . .\")\n","      clear_output(wait=True)\n","    else:\n","      display(plt.gcf())\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cd_IKm4G9x-B"},"source":["## **Creating the Optimizing function:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BF8Kj1Y6tZsw"},"outputs":[],"source":["def optimize_model():\n","  if replay_memory.length() < BATCH_SIZE:            #maintaining a minimum batch size to be used for optimizing function\n","    return\n","  transitions = replay_memory.sample(BATCH_SIZE)     #sampling random episodes to train from the replay_memory\n","  batch = transition(*zip(*transitions))             #maintaining a batch of all the required info to access directly from the initial namedtuple\n","  mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)        #a mask of type torch-tensor-boolean which tells if the state has a next_state or not\n","\n","  next_states = torch.cat([s for s in batch.next_state if s is not None])\n","  state_batch = torch.cat(batch.state)\n","  action_batch = torch.cat(batch.action)\n","  reward_batch = torch.cat(batch.reward)\n","\n","  state_action_values = online_net(state_batch).gather(1, action_batch)         #gather all the action values produced by the DQN, when given the state_batch\n","\n","  next_state_values = torch.zeros(BATCH_SIZE)\n","\n","  with torch.no_grad():\n","    next_state_values[mask] = target_net(next_states).max(1).values\n","\n","\n","  expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","  criterion = nn.SmoothL1Loss()                                                                 #calculating the huber_loss\n","  loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","  optimizer.zero_grad()\n","  loss.backward()                                                                       #calculating the loss and optimizing the network to minimize the loss\n","  torch.nn.utils.clip_grad_value_(online_net.parameters(), 100)\n","  optimizer.step()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9r8J9PLS-CyZ"},"source":["## **training the Online_Network:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5KuXVjbaROM"},"outputs":[],"source":["from types import new_class\n","iterations_epi = 3000\n","\n","\n","for i in range(iterations_epi):\n","  state, info = env.reset()\n","  state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)              #convert the state into a state tensor\n","\n","  total_reward = 0\n","\n","  for t in count():\n","    action = action_selection(state)                                             # select an action based on the state\n","    observation, reward, terminated, truncated, _ = env.step(action.item())\n","    reward = torch.tensor([reward])\n","\n","    total_reward += reward.item()\n","\n","    if terminated:\n","      next_state = None\n","\n","    else:\n","      next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)            #the next_state is the observation after the action is completed\n","\n","\n","    replay_memory.push(state, action, next_state, reward)\n","\n","    state = next_state\n","\n","    optimize_model()\n","\n","    #soft updating the weights in target net using the weights from the online_net\n","    target_net_state_dict = target_net.state_dict()\n","    online_net_state_dict = online_net.state_dict()\n","    for key in online_net_state_dict:\n","      target_net_state_dict[key] = online_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n","\n","    if (terminated or truncated):\n","      episode_rewards.append(total_reward)\n","      break\n","print('Done')\n","plot_rewards(show_result=True)\n","plt.ioff()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsaRWnZokQ8m"},"outputs":[],"source":["pip install swig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYTVIGOVkS_-"},"outputs":[],"source":["pip install gymnasium[box2D]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT3oLU2sl9GJ"},"outputs":[],"source":["pip install imageio-ffmpeg"]},{"cell_type":"markdown","source":["## **Rendering the game:**"],"metadata":{"id":"AX7J8NTio5cw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5Xjq0r9wITa"},"outputs":[],"source":["import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","import gymnasium as gym\n","import torch\n","\n","def show_video_of_model(agent, env_name):\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    state, _ = env.reset()\n","    done = False\n","    frames = []\n","    while not done:\n","        frame = env.render()\n","        frames.append(frame)\n","        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","        action = action_selection(state_tensor)\n","        state, reward, done, _, _ = env.step(action.item())\n","    env.close()\n","    imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(online_net, \"ALE/Tetris-ram-v5\")\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data=f'''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n","                              <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n","                             </video>'''))\n","    else:\n","        print(\"Could not find video\")\n","\n","show_video()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Sx9v4goC2Zz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AGPJXNmC5q3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNXqCsetjINA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5C_Gg5l_2vB1"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"11tKMkpDOq_KEoiff63GcZKMKsW4VNpQV","timestamp":1737308382634}],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}